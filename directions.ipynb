{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8b525cd910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "from neel_plotly import line, imshow, scatter\n",
    "from jaxtyping import Float\n",
    "import plotly.io as pio\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import transformer_lens.patching as patching\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import get_batched_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_devices = torch.cuda.device_count()\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2b\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=False,\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    #n_devices=2\n",
    ")\n",
    "model.set_use_attn_result(True)\n",
    "# Get the default device used\n",
    "device: torch.device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import load_sva_dataset\n",
    "\n",
    "dataset_type = 'both' # singular / plural / both\n",
    "language = 'english' # english / spanish / both\n",
    "num_samples = 200\n",
    "batch_size = 10\n",
    "start_at = 0\n",
    "dataset = load_sva_dataset(model, language, dataset_type, num_samples)\n",
    "batches_base_tokens, batches_src_tokens, batches_answer_token_indices = get_batched_dataset(model,\n",
    "                                                                                            dataset['base_list'],\n",
    "                                                                                            dataset['src_list'],\n",
    "                                                                                            dataset['answers'],\n",
    "                                                                                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting steering vectors (PCA directions or difference in means)\n",
    "tensors_dir = f\"{home}/mats/sva_tensors\"\n",
    "attn_layer_index = 13\n",
    "attn_head_index = 7\n",
    "hook_name = utils.get_act_name('result', attn_layer_index)\n",
    "method = 'pca' # diff_means / pca\n",
    "tensors_dict_ = {}\n",
    "with safe_open(f\"{tensors_dir}/{model.cfg.model_name}_both_english_{hook_name}_{method}_singular_plural.safetensors\", framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            tensors_dict_[k] = f.get_tensor(k)\n",
    "if method == 'pca':\n",
    "      # pca_component, head_index, d_repre\n",
    "      direction = tensors_dict_['directions'][0][attn_head_index]\n",
    "elif method == 'diff_means':\n",
    "      direction = tensors_dict_['direction'][attn_head_index]\n",
    "\n",
    "\n",
    "# adding 8*(PCA 1st component) flips prediction in singular (+) and plural (-)\n",
    "# adding 8*(diff_means) flips prediction in singular (-) and plural (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logit_list = []\n",
    "zeroed_logit_list = []\n",
    "patched_logit_list = []\n",
    "\n",
    "def patch_hook(acts, hook, attn_head_index, steering_vector):\n",
    "            # Adding steering vector to hook\n",
    "            # [batch, pos, head_index, d_model]\n",
    "            steering_vector = steering_vector / steering_vector.norm()\n",
    "            new_acts = acts.clone()\n",
    "            mean_head_norm = new_acts[:,-1,attn_head_index].norm(dim=-1).mean().item()\n",
    "            #print('Orig attn out norm', mean_head_norm)\n",
    "            #print('steering vector norm', steering_vector.norm())\n",
    "            # print('attn output shape', new_acts[:,-1,attn_head_index].shape)\n",
    "            # project out the subject number feature in attn_head_index output\n",
    "            #new_acts[:,-1,attn_head_index] = new_acts[:,-1,attn_head_index] - (new_acts[:,-1,attn_head_index] @ steering_vector)[..., None] * steering_vector\n",
    "            # Project out on every head output\n",
    "            #new_acts[:,-1,:] = new_acts[:,-1,:] - (new_acts[:,-1,:] @ steering_vector)[..., None] * steering_vector\n",
    "\n",
    "            new_acts[:,-1,attn_head_index] -= mean_head_norm*steering_vector.unsqueeze(0) #torch.zeros([1,2048]).to(device)\n",
    "            return new_acts\n",
    "\n",
    "def zero_hook(acts, hook, attn_head_index):\n",
    "            # Adding steering vector to hook\n",
    "            new_acts = acts.clone()\n",
    "            #print('hey', new_acts.shape)\n",
    "            new_acts[:,-1,attn_head_index,:] = torch.zeros([1,1,2048]).to(device)\n",
    "            return new_acts\n",
    "\n",
    "patch_hook_fn = partial(patch_hook, attn_head_index=attn_head_index, steering_vector=direction.to(device))\n",
    "zero_hook_fn = partial(zero_hook, attn_head_index=attn_head_index)\n",
    "hook_to_steer = hook_name#utils.get_act_name('result', attn_layer_index)#utils.get_act_name('resid_mid', 13)\n",
    "hook_to_zero = hook_name#utils.get_act_name('result', attn_layer_index)\n",
    "\n",
    "for batch in range(batchs):\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "    # Clean run\n",
    "    base_logits = model(batches_base_tokens[batch])\n",
    "    base_logits_idxs = base_logits[0, -1, :].topk(20, dim=-1).indices\n",
    "    logit_toks = [model.to_str_tokens(x) for x in base_logits_idxs]\n",
    "    print('before', logit_toks)\n",
    "\n",
    "    # Zero ablation run (zero ablate attention head)\n",
    "    zeroed_logits = model.run_with_hooks(batches_base_tokens[batch],\n",
    "                            return_type=\"logits\",\n",
    "                            fwd_hooks=[(hook_to_zero, zero_hook_fn)]\n",
    "                            )\n",
    "\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "    # Patched (steered) run\n",
    "    patched_logits = model.run_with_hooks(batches_base_tokens[batch],\n",
    "                            return_type=\"logits\",\n",
    "                            fwd_hooks=[(hook_to_steer, patch_hook_fn)]#,\n",
    "                                        #(hook_to_zero, zero_hook_fn)]\n",
    "                            )\n",
    "    patched_logits_idxs = patched_logits[0, -1, :].topk(20, dim=-1).indices\n",
    "    patched_toks = [model.to_str_tokens(x) for x in patched_logits_idxs]\n",
    "    print('after', patched_toks)\n",
    "\n",
    "    answer_token_indices = batches_answer_token_indices[batch]\n",
    "    answer_token_indices = answer_token_indices.to(base_logits.device)\n",
    "\n",
    "    base_logit_diff = get_logit_diff(base_logits, answer_token_indices).item()\n",
    "    clean_logit_list.append(base_logit_diff)\n",
    "\n",
    "    zeroed_logit_diff = get_logit_diff(zeroed_logits, answer_token_indices).item()\n",
    "    zeroed_logit_list.append(zeroed_logit_diff)\n",
    "\n",
    "    patched_logit_diff = get_logit_diff(patched_logits, answer_token_indices, mean=True).item()\n",
    "    # condition = patched_logit_diff > 0.\n",
    "    # row_cond = condition.all(1)\n",
    "    patched_logit_list.append(patched_logit_diff)\n",
    "\n",
    "print(f\"Base logit diff: {np.array(clean_logit_list).mean():.4f}\")\n",
    "#print(f\"Zero ablation logit diff: {np.array(zeroed_logit_list).mean():.4f}\")\n",
    "print(f\"Patched logit diff: {np.array(patched_logit_list).mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.reset_hooks(including_permanent=True)\n",
    "# model.add_perma_hook(hook_to_steer, patch_hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_probs = base_logits.softmax(dim=-1)[:,-1]\n",
    "clean_answer_probs = clean_probs.gather(-1, answer_token_indices[:,0].unsqueeze(1)).squeeze()\n",
    "\n",
    "patched_probs = patched_logits.softmax(dim=-1)[:,-1]\n",
    "patched_answer_probs = patched_probs.gather(-1, answer_token_indices[:,0].unsqueeze(1)).squeeze()\n",
    "\n",
    "from rich import print as rprint\n",
    "from rich.table import Table, Column\n",
    "cols = [\n",
    "    \"Prompt\",\n",
    "    Column(\"Correct\", style=\"rgb(0,200,0) bold\"),\n",
    "    #Column(\"Incorrect\", style=\"rgb(255,0,0) bold\"),\n",
    "    Column(\"Prob correct\", style=\"bold\"),\n",
    "    Column(\"Patched prob correct\", style=\"bold\")\n",
    "]\n",
    "table = Table(*cols, title=\"Logit differences\")\n",
    "\n",
    "for prompt, answer, prob_clean_answer, prob_patched_answer in zip(batches_base_list[batch], batches_answers[batch], clean_answer_probs, patched_answer_probs):\n",
    "    table.add_row(prompt, repr(answer[0]), f\"{prob_clean_answer.item():.3f}\", f\"{prob_patched_answer.item():.3f}\")\n",
    "\n",
    "rprint(table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
